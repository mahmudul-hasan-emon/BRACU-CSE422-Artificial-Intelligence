# -*- coding: utf-8 -*-
"""6_19101098_MahmudulHasanEmon_All ML Labs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UEH0kNlynKp-2P-qaWUWSrrJszukqnh_

**Assignment-4: Data preprocessing**

**Task-03: Load the dataset as dataframe using pandas**
"""

import pandas as pd

wine = pd.read_csv("/content/wine.csv")

"""**Data visualization**"""

wine.shape

wine.head() # default = 5

wine.sample() # default = 1

wine.columns

"""**Task-04: Handle missing values (if needed)**

* Null values
* Missing values
* NaN values
"""

# True  : if cell value is NaN
# False : if cell value is not NaN

wine.isnull()

# column_name frequency_of_nan_values

wine.isnull().sum()

# Impute missing values-01

#No NaN values found in this "wine dataset" that's why did not handle missing values.

"""**Task-05: Encode categorical features (if needed)**"""

wine.info()

#enconde categorical features
# Encode-01
# built-in class 


from sklearn.preprocessing import LabelEncoder
# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
wine['quality_enc'] = enc.fit_transform(wine['quality'])

# Compare the two columns
print(wine[['quality','quality_enc']].head(12))

"""**Task-06: Scale all the values between 0-1 with proper scaling technique**

**MinMax Scaler:**

Scales values to a range between 0 and 1 if no negative values, and -1 to 1 if there are negative values present.
"""

print(wine["free sulfur dioxide"].max())
print(wine["free sulfur dioxide"].min())

from sklearn.preprocessing import MinMaxScaler

list_of_features = ["free sulfur dioxide"]

scaler = MinMaxScaler()

scaler.fit(wine[list_of_features])

scaled_data = scaler.transform(wine[list_of_features])

print(scaled_data.max())

print(scaled_data.min())

scaled_data

"""**Task-07: Split the dataset into features and labels. (Use your intuition to determine which column indicates the labels.)**"""

from sklearn.model_selection import train_test_split

list_of_features = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']
x_data = wine[list_of_features]
y_data = wine["quality_enc"]

X_train, X_test, y_train, y_test = train_test_split(x_data, y_data,random_state=1)

"""**Assignment-5: Regression**"""

# Import the dependencies for logistic regression
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#Train the model
model = LogisticRegression()
model.fit(X_train, y_train) #Training the model
predictions = model.predict(X_test)
print(predictions)# printing predictions

lr= LogisticRegression()
lr.fit(X_train, y_train)
y_predict_lr= lr.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy_lr= accuracy_score(y_test, y_predict_lr)
accuracy_lr

from sklearn.tree import DecisionTreeClassifier
dtc= DecisionTreeClassifier(criterion='entropy', random_state=1)
dtc.fit(X_train, y_train)
y_predict_dtc=dtc.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy_dtc= accuracy_score(y_test, y_predict_dtc)
accuracy_dtc

import matplotlib.pyplot as plt
plt.figure(figsize=(10,10))
classifiers = ['Logistic Regression', 'Decision Tree']
accuracy= [accuracy_lr, accuracy_dtc]
plt.bar(classifiers, accuracy)
plt.show()

"""**`Assignment-6:`**"""

from sklearn.svm import SVC
m1= SVC(kernel="linear")
m1.fit(X_train,y_train)
y_pred1= m1.predict(X_test)
score1 = accuracy_score(y_pred1,y_test)
score1

from sklearn.neural_network import MLPClassifier
m2 = MLPClassifier(hidden_layer_sizes=(7), activation="relu",max_iter=10000)
m2.fit(X_train,y_train)
y_pred2= m2.predict(X_test)
score2= accuracy_score(y_pred2,y_test)
score2

from sklearn.ensemble import RandomForestClassifier
m3 = RandomForestClassifier(n_estimators=50)
m3.fit(X_train,y_train)
y_pred3= m3.predict(X_test)
score3= accuracy_score(y_pred3,y_test)
score3

half = (wine.columns.shape[0]-1)//2
half

from sklearn.decomposition import PCA
pca = PCA(n_components=half)

principal_components = pca.fit_transform(x_data)
principal_components

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

principal_df=pd.DataFrame(data=principal_components,columns=["pc1","pc2","pc3","pc4","pc5","pc6"])

main_df=pd.concat([principal_df,wine[["quality_enc"]]],axis=1)
main_df.head()

x_data= main_df.drop("quality_enc", axis=1)
y_dat= main_df["quality_enc"]
X_train,X_test, y_train,y_test= train_test_split(x_data,y_data,test_size=0.2,random_state=1)

from sklearn.svm import SVC
m1= SVC(kernel="linear")
m1.fit(X_train,y_train)
y_pred1= m1.predict(X_test)
score11= accuracy_score(y_pred1,y_test)
score11

from sklearn.neural_network import MLPClassifier
m2 = MLPClassifier(hidden_layer_sizes=(7), activation="relu",max_iter=10000)
m2.fit(X_train,y_train)
y_pred2= m2.predict(X_test)
score22= accuracy_score(y_pred2,y_test)
score22

from sklearn.ensemble import RandomForestClassifier
m3 = RandomForestClassifier(n_estimators=50)
m3.fit(X_train,y_train)
y_pred3= m3.predict(X_test)
score33= accuracy_score(y_pred3,y_test)
score33

import numpy as np
import matplotlib.pyplot as plt
 
# set width of bar
barWidth = 0.20
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar
svm = [score1, score11]
mlp = [score2, score22]
rfc = [score3, score33]
 
# Set position of bar on X axis
br1 = np.arange(len(svm))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, svm, color ='r', width = barWidth,
        edgecolor ='grey', label ='SVM')
plt.bar(br2, mlp, color ='g', width = barWidth,
        edgecolor ='grey', label ='MLP')
plt.bar(br3, rfc, color ='b', width = barWidth,
        edgecolor ='grey', label ='RFC')
 
# Adding Xticks
plt.ylabel('Score', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(svm))],
        ['Before-PCA', 'After-PCA'])
 
plt.legend()
plt.show()